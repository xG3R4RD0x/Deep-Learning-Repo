{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "First we need to load the MNIST dataset from disk. Since in this exercise we are doing binary classification, i.e. classification between two classes, we only pick the digits 1 and 8 from the MNIST dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import edf\n",
    "import mnist_loader\n",
    "from skimage.transform import resize\n",
    "np.random.seed(1234)\n",
    "\n",
    "x_train, y_train = mnist_loader.load_mnist(section = 'training', path = 'MNIST')\n",
    "x_test, y_test = mnist_loader.load_mnist(section = 'testing', path = 'MNIST')\n",
    "\n",
    "digits = [1, 8]\n",
    "train_subset_mask = np.logical_or(y_train == digits[0], y_train == digits[1])\n",
    "x_train = x_train[train_subset_mask]\n",
    "y_train = y_train[train_subset_mask]\n",
    "y_train[y_train == digits[0]] = 0\n",
    "y_train[y_train == digits[1]] = 1\n",
    "test_subset_mask = np.logical_or(y_test == digits[0], y_test == digits[1])\n",
    "x_test = x_test[test_subset_mask]\n",
    "y_test = y_test[test_subset_mask]\n",
    "y_test[y_test == digits[0]] = 0\n",
    "y_test[y_test == digits[1]] = 1\n",
    "\n",
    "plt.imshow(x_train[0], cmap='gray', interpolation = 'nearest')\n",
    "plt.show()\n",
    "plt.imshow(x_train[600], cmap='gray', interpolation = 'nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating downsampled versions of the dataset\n",
    "\n",
    "In the end of this exercise we explore how the classification accuracy depends on the input image resolution. The code below creates 4 different versions of the dataset. This takes some time to execute. During your development you only need to execute this cell and the above one once and then you can modify and test your implementations below with the dataset already beeing loaded. (big advantage of jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resize_images(images, res_x, res_y):\n",
    "    resized_images = np.zeros((images.shape[0], res_x, res_y))\n",
    "    for n in range(len(images)):\n",
    "        resized_images[n, :, :] = resize(images[n, :, :], resized_images.shape[1:], anti_aliasing=False)\n",
    "    return resized_images\n",
    "\n",
    "# The 28x28 images are flattend into a feature (=input) vector of size 784 here\n",
    "x_train_full_res = x_train.reshape(x_train.shape[0], -1) # flatten\n",
    "x_test_full_res = x_test.reshape(x_test.shape[0], -1) # flatten\n",
    "print(x_train_full_res.shape, x_test_full_res.shape)\n",
    "\n",
    "# The means of the flattend images is taken. So in this case we only have a single input feature\n",
    "x_train_mean = np.mean(x_train_full_res, axis=1, keepdims=True)\n",
    "x_test_mean = np.mean(x_test_full_res, axis=1, keepdims=True)\n",
    "print(x_train_mean.shape, x_test_mean.shape)\n",
    "\n",
    "# 28x28 images are downscaled into 4x4 images and then flattened to arrive at 16 features\n",
    "x_train_4x4 = resize_images(x_train, 4, 4).reshape(x_train.shape[0], -1)\n",
    "x_test_4x4 = resize_images(x_test, 4, 4).reshape(x_test.shape[0], -1)\n",
    "print(x_train_4x4.shape, x_test_4x4.shape)\n",
    "\n",
    "# 28x28 images are downscaled into 8x8 images and then flattened to arrive at 64 features\n",
    "x_train_8x8 = resize_images(x_train, 8, 8).reshape(x_train.shape[0], -1)\n",
    "x_test_8x8 = resize_images(x_test, 8, 8).reshape(x_test.shape[0], -1)\n",
    "print(x_train_8x8.shape, x_test_8x8.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression based on hand derived derivative\n",
    "Implement the function `compute_derivative`. It often helps to print the `shape` of numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivative(x, y, y_hat):\n",
    "    pass\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def Xavier(num_features):\n",
    "    return np.sqrt(3.0 / num_features)\n",
    "\n",
    "def analytical_train_and_test(num_epochs, batch_size, learning_rate, x_train, y_train, x_test, y_test):\n",
    "    # initalize our parameters w randomly\n",
    "    num_features = x_train.shape[1]\n",
    "    xavier_init = Xavier(num_features) # \n",
    "    w = np.random.uniform(-xavier_init, xavier_init, size=(num_features + 1,))\n",
    "    w[-1] = 0 # set bias term to zero\n",
    "    \n",
    "    train_err_log = []\n",
    "    test_err_log = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "        train_err = analytical_run_epoch(batch_size, x_train, y_train, w, 'train', learning_rate=learning_rate)\n",
    "        train_err_log.append(train_err)\n",
    "        print (\"\\t Training Error {:.2f} %\".format(train_err))\n",
    "        test_err = analytical_run_epoch(x_test.shape[0], x_test, y_test, w, 'test')\n",
    "        test_err_log.append(test_err)\n",
    "        print (\"\\t Test Error {:.2f} %\".format(test_err))\n",
    "    return train_err_log, test_err_log\n",
    "    \n",
    "def analytical_run_epoch(batch_size, x, y, w, phase, learning_rate=None):\n",
    "    dataset_size = len(x)\n",
    "    total_err = 0.0\n",
    "    num_batches = dataset_size // batch_size\n",
    "    for i in range(num_batches):\n",
    "        start, end = i * batch_size, (i + 1) * batch_size\n",
    "        x_batch = x[start:end]\n",
    "        y_batch = y[start:end]\n",
    "        one_column_vector = np.ones((x_batch.shape[0], 1)) # a column vector with all entries beeing 1\n",
    "        x_batch = np.concatenate((x_batch, one_column_vector), axis=1) # append this column vector to x\n",
    "        y_hat = sigmoid(x_batch.dot(w))\n",
    "        if phase == 'train':\n",
    "            w_derivative = compute_derivative(x_batch, y_batch, y_hat)\n",
    "            w[:] = w - learning_rate * w_derivative\n",
    "        prediction = (y_hat > 0.5)\n",
    "        total_err += np.sum(np.not_equal(prediction, y_batch))\n",
    "    return 100 * total_err / dataset_size\n",
    "\n",
    "def plot(train_err_log, test_err_log):\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"error (%)\")\n",
    "    plt.plot(np.arange(len(test_err_log)), test_err_log, color='red')\n",
    "    plt.plot(np.arange(len(train_err_log)), train_err_log, color='blue')\n",
    "    plt.legend(['test error', 'train error'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.5\n",
    "train_err_log, test_err_log = analytical_train_and_test(num_epochs, batch_size, learning_rate,\n",
    "                                             x_train_mean, y_train, x_test_mean, y_test)\n",
    "plot(train_err_log, test_err_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises based on EDF\n",
    "\n",
    "Implement the `forward` and `backward` method of this class. Take a look at `edf.py` for inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape: (batch size, 1)\n",
    "# output shape: (batch size, 2)\n",
    "class SingleProbToProbVector(edf.CompNode):\n",
    "    def __init__(self, z):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.z = z\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression based on EDF and mean images\n",
    "\n",
    "Use EDF to assemble a computational graph for logistic regression with the \"mean\" images as input. You will need the `SingleProbToProbVector` node that you implemented above. The function `train_and_test` below expects variables `x_node`, `y_node`, `prob_node` and `loss_node` to be defined. `prob_node` calculates the class probabilities for each training sample, while the `loss_node` calculates the loss for the entire training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf.clear_compgraph()\n",
    "# Define the computation graph here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the following functions are used to train the network.\n",
    "training is done by iterating over mini-batches of size 'batch_size'\n",
    "and updating the model's parameters with SGD\"\"\"\n",
    "\n",
    "def run_epoch(batch_size, x, y, x_node, y_node, prob_node, loss_node=None):\n",
    "    dataset_size = x.shape[0]\n",
    "    total_err = 0.0\n",
    "    num_batches = dataset_size // batch_size\n",
    "    for i in range(num_batches):\n",
    "        start, end = i * batch_size, (i + 1) * batch_size\n",
    "        x_node.value = x[start:end]\n",
    "        y_node.value = y[start:end]\n",
    "        edf.Forward()\n",
    "        total_err += np.sum(np.not_equal(np.argmax(prob_node.value, axis=1), y_node.value))\n",
    "        if loss_node:\n",
    "            edf.Backward(loss_node)\n",
    "            edf.UpdateParameters()\n",
    "    return 100 * total_err / dataset_size\n",
    "\n",
    "def train_and_test(num_epochs, batch_size, x_train, y_train, x_test, y_test,\n",
    "                   x_node, y_node, prob_node, loss_node):\n",
    "    train_err_log = []\n",
    "    test_err_log = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "        train_err = run_epoch(batch_size, x_train, y_train, x_node, y_node, prob_node, loss_node)\n",
    "        train_err_log.append(train_err)\n",
    "        print (\"\\t Training Error {:.2f} %\".format(train_err))\n",
    "        test_err = run_epoch(len(x_test), x_test, y_test, x_node, y_node, prob_node)\n",
    "        test_err_log.append(test_err)\n",
    "        print (\"\\t Test Error {:.2f} %\".format(test_err))\n",
    "    return train_err_log, test_err_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now, we are ready to train the network. we can choose SGD's learning rate\n",
    "by changing edf.learning_rate, which we will set as 0.5 for now.\"\"\"\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.5\n",
    "train_err_log, test_err_log = train_and_test(num_epochs, batch_size, x_train_mean, y_train, x_test_mean, y_test,\n",
    "                                             x_node, y_node, prob_node, loss_node)\n",
    "plot(train_err_log, test_err_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4x4, 8x8 and full resolution experiments based on EDF\n",
    "\n",
    "Define computational graphs for the 4x4, 8x8 and full resolution experiments and run them. You need to call `edf.clear_compgraph()` before defining a new computation graph.\n",
    "\n",
    "### 4x4 experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8x8 experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full res (=28x28) experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
