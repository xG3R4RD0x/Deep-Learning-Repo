{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "First we need to load the MNIST dataset from disk. We will do 10-class classification for digit 0, 1, .., 9 from the MNIST dataset here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import edf\n",
    "import mnist_loader\n",
    "\n",
    "train_images, train_labels = mnist_loader.load_mnist(section = 'training', path = 'MNIST')\n",
    "test_images, test_labels = mnist_loader.load_mnist(section = 'testing', path = 'MNIST')\n",
    "\n",
    "plt.imshow(train_images[0], cmap='gray', interpolation = 'nearest')\n",
    "plt.show()\n",
    "\n",
    "# quickly check the shape of data\n",
    "print('Train data shape: ', train_images.shape)\n",
    "print('Train labels shape: ', train_labels.shape)\n",
    "print('Test data shape: ', test_images.shape)\n",
    "print('Test labels shape: ', test_labels.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"Preprocessing: we reshape the 28x28 grayscale MNIST images to \n",
    "784-dimensional vectors, which will be the network's inputs\"\"\"\n",
    "\n",
    "train_images = train_images.reshape(len(train_images), -1)\n",
    "test_images = test_images.reshape(len(test_images), -1)\n",
    "\n",
    "# now check the shape of data after reshaping\n",
    "print('Train data shape: ', train_images.shape)\n",
    "print('Train labels shape: ', train_labels.shape)\n",
    "print('Test data shape: ', test_images.shape)\n",
    "print('Test labels shape: ', test_labels.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "## Logistic Regression for 10-Class Image Classification\n",
    "\n",
    "Use EDF to assemble a computational graph for logistic regression with the softmax function. You can reuse your code from previous exercises, but no need to use `SingleProbToProbVector` class anymore because we are now tackling image classification task on **10** different classes, instead of 2 classes. Moreover, please replace the output function that you used before from `Sigmoid` to `Softmax`.\n",
    "\n",
    "Note that we don't need to implment the backward pass because the backward function of `Affine`, `Sigmoid` and `Softmax` have been implemented in EDF."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nInputs = train_images.shape[1] # 784-dimension \n",
    "nOutputs = 10 # Output dimension\n",
    "\n",
    "edf.clear_compgraph()\n",
    "\n",
    "# TO-DO: Define the computation graph here\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"the following functions are used to train the network.\n",
    "training is done by iterating over mini-batches of size 'batch_size'\n",
    "and updating the model's parameters with SGD\"\"\"\n",
    "\n",
    "def run_epoch(batch_size, data, labels, x_node, y_node, prob_node, loss_node=None):\n",
    "    num_samples = len(data)\n",
    "    total_err = 0.0\n",
    "    num_batches = num_samples//batch_size\n",
    "    for i in range(num_batches):\n",
    "        start, end = i*batch_size, (i+1)*batch_size\n",
    "        x_node.value = train_images[start:end]\n",
    "        y_node.value = train_labels[start:end]\n",
    "        edf.Forward()\n",
    "        total_err += np.sum(np.not_equal(np.argmax(prob_node.value, axis=1), y_node.value))\n",
    "        if loss_node:\n",
    "            edf.Backward(loss_node)\n",
    "            edf.UpdateParameters()\n",
    "        if i>0 and i%400 == 0:\n",
    "            print (\"\\t Batch {}/{}\".format(i, num_batches))\n",
    "    return 100*total_err/num_samples\n",
    "\n",
    "def train_and_test(num_epochs, batch_size, x_node, y_node, prob_node, loss_node):\n",
    "    train_err_log = []\n",
    "    test_err_log = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}/{} (learning rate: {})\".format(epoch+1, num_epochs, edf.learning_rate))\n",
    "        train_err = run_epoch(batch_size, train_images, train_labels, x_node, y_node, prob_node, loss_node)\n",
    "        train_err_log.append(train_err)\n",
    "        print (\"\\t Training Error {:.2f} %\".format(train_err))\n",
    "        test_err = run_epoch(len(test_images), test_images, test_labels, x_node, y_node, prob_node)\n",
    "        test_err_log.append(test_err)\n",
    "        print (\"\\t Test Error {:.2f} %\".format(test_err))\n",
    "    return train_err_log, test_err_log\n",
    "\n",
    "\"\"\"plot function\"\"\"\n",
    "def plot(train_err_log, test_err_log):\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"error (%)\")\n",
    "    plt.plot(np.arange(len(test_err_log)), test_err_log, color='red')\n",
    "    plt.plot(np.arange(len(train_err_log)), train_err_log, color='blue')\n",
    "    plt.legend(['test error', 'train error'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"now, we are ready to train the logistic regression. we can choose SGD's learning rate\n",
    "by changing edf.learning_rate, which we will set as 0.5 for now.\"\"\"\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.5\n",
    "train_err_log, test_err_log = train_and_test(num_epochs, batch_size, x_node, y_node, prob_node, loss_node)\n",
    "plot(train_err_log, test_err_log)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "### Inline Question 1:\n",
    "If your logistic regression model weight is randomly initalized, and no training is done as above. What test error do you think the model will get?\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Image Classification with MLP \n",
    "\n",
    "You may notice that the logistic regression model resulted with relatively high errors (~8%). Now it is time to see the power of neural network :)  \n",
    "\n",
    "You now should develop a multi-layer perceptron with two layers.\n",
    "Use EDF to assemble a computational graph for image classification using MLP with the vectorized images as input. Similar to the logistic regression task, the function `train_and_test` below expects variables `x_node`, `y_node`, `prob_node` and `loss_node` to be defined. `prob_node` calculates the class probabilities for each training sample, while we use `CrossEntropyLoss` for the `loss_node` to calculate the loss for the entire training batch. `Softmax` should be also used for the output."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nInputs = train_images.shape[1] # 784-dimension \n",
    "nOutputs = 10 # Output dimension\n",
    "nHiddens = 64 # Number of neurons in the hidden layer\n",
    "\n",
    "edf.clear_compgraph()\n",
    "\n",
    "# TO-DO: Define the computation graph here\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"now, we are ready to train the network. we can choose SGD's learning rate\n",
    "by changing edf.learning_rate, which we will set as 0.5 for now.\"\"\"\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.5\n",
    "train_err_log, test_err_log = train_and_test(num_epochs, batch_size, x_node, y_node, prob_node, loss_node)\n",
    "plot(train_err_log, test_err_log)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "# Architecture Choice\n",
    "\n",
    "## Activation functions\n",
    "We want to explore the performance when using different activation function.\n",
    "\n",
    "First, you will implement a `Tanh` activation function by filling the missing\n",
    "code in the `forward` and `backward` methods below. You can re-use parts of EDF's Sigmoid code.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Tanh(edf.CompNode):\n",
    "    def __init__(self, x):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        # To-DO: implement the forward pass\n",
    "\n",
    "    def backward(self):\n",
    "        # To-DO: implement the backward pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"the code below will plot the output and gradients computed by your\n",
    "implementation of the Tanh component above. check if the plots match\n",
    "Tanh(x) and dTanh/dx(x) as a sanity test of your implementation.\"\"\"\n",
    "\n",
    "values = np.linspace(-5,5,100)\n",
    "edf.clear_compgraph()\n",
    "params = edf.Parameter(values[None, :])\n",
    "output = Tanh(params)\n",
    "\n",
    "edf.Forward()\n",
    "edf.Backward(output)\n",
    "\n",
    "plt.xlabel(\"value\")\n",
    "plt.plot(values, output.value[0], color='red')\n",
    "plt.plot(values, params.grad[0], color='blue')\n",
    "plt.legend(['output', 'grad'], loc='upper left')\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "Again, assemble a computational graph for image classification using MLP with `Tanh` activation function. The function `train_and_test` below expects variables `x_node`, `y_node`, `prob_node` and `loss_node` to be defined."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nInputs = train_images.shape[1] # 784-dimension \n",
    "nOutputs = 10 # Output dimension\n",
    "nHiddens = 64 # Number of neurons in the hidden layer\n",
    "\n",
    "np.random.seed(1234)\n",
    "edf.clear_compgraph()\n",
    "\n",
    "# TO-DO: Define the computation graph here\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"now, we are ready to train the Tanh MLP network. \"\"\"\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.5\n",
    "train_err_log, test_err_log = train_and_test(num_epochs, batch_size, x_node, y_node, prob_node, loss_node)\n",
    "plot(train_err_log, test_err_log)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "Now, you will implement a ReLU activation function ReLU(x) = max(0,x).\n",
    "implement the `forward` and `backward` methods of the following class.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ReLU(edf.CompNode):\n",
    "    def __init__(self, x):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        # To-DO: implement the forward pass\n",
    "\n",
    "    def backward(self):\n",
    "        # To-DO: implement the backward pass\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"the code below will plot the output and gradients computed by your\n",
    "implementation of the ReLU component above. check if the plots match\n",
    "ReLU(x) and dReLU/dx(x) as a sanity test of your implementation.\"\"\"\n",
    "\n",
    "values = np.linspace(-2,2,100)\n",
    "edf.clear_compgraph()\n",
    "params = edf.Parameter(values[None, :])\n",
    "output = ReLU(params)\n",
    "\n",
    "edf.Forward()\n",
    "edf.Backward(output)\n",
    "\n",
    "plt.xlabel(\"value\")\n",
    "plt.plot(values, output.value[0], color='red')\n",
    "plt.plot(values, params.grad[0], color='blue')\n",
    "plt.legend(['output', 'grad'], loc='upper left')\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "Again, assemble a computational graph for image classification using MLP with `ReLU` activation function. The function `train_and_test` below expects variables `x_node`, `y_node`, `prob_node` and `loss_node` to be defined."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nInputs = train_images.shape[1] # 784-dimension \n",
    "nOutputs = 10 # Output dimension\n",
    "nHiddens = 64 # Number of neurons in the hidden layer\n",
    "\n",
    "np.random.seed(1234)\n",
    "edf.clear_compgraph()\n",
    "\n",
    "# TO-DO: Define the computation graph here\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"now, we are ready to train the ReLU MLP network. \"\"\"\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.5\n",
    "train_err_log, test_err_log = train_and_test(num_epochs, batch_size, x_node, y_node, prob_node, loss_node)\n",
    "plot(train_err_log, test_err_log)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "Now, you will implement a LeakyReLU(x) = max(cx, x) with the constant `c` = 0.01.\n",
    "implement the `forward` and `backward` methods of the following class.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LeakyReLU(edf.CompNode):\n",
    "    def __init__(self, x):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "        self.c = 0.01\n",
    "\n",
    "    def forward(self):\n",
    "        # To-DO: implement the forward pass\n",
    "\n",
    "    def backward(self):\n",
    "        # To-DO: implement the backward pass\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"the code below will plot the output and gradients computed by your\n",
    "implementation of the ReLU component above. check if the plots match\n",
    "LeakyReLU(x) and d LeakyReLU/dx(x) as a sanity test of your implementation.\"\"\"\n",
    "\n",
    "values = np.linspace(-2,2,100)\n",
    "edf.clear_compgraph()\n",
    "params = edf.Parameter(values[None, :])\n",
    "output = LeakyReLU(params)\n",
    "\n",
    "edf.Forward()\n",
    "edf.Backward(output)\n",
    "\n",
    "plt.xlabel(\"value\")\n",
    "plt.plot(values, output.value[0], color='red')\n",
    "plt.plot(values, params.grad[0], color='blue')\n",
    "plt.legend(['output', 'grad'], loc='upper left')\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "Again, assemble a computational graph for image classification using MLP with `LeakyReLU` activation function. The function `train_and_test` below expects variables `x_node`, `y_node`, `prob_node` and `loss_node` to be defined."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nInputs = train_images.shape[1] # 784-dimension \n",
    "nOutputs = 10 # Output dimension\n",
    "nHiddens = 64 # Number of neurons in the hidden layer\n",
    "\n",
    "np.random.seed(1234)\n",
    "edf.clear_compgraph()\n",
    "\n",
    "# TO-DO: Define the computation graph here\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"now, we are ready to train the ReLU MLP network. \"\"\"\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "edf.learning_rate = 0.5\n",
    "train_err_log, test_err_log = train_and_test(num_epochs, batch_size, x_node, y_node, prob_node, loss_node)\n",
    "plot(train_err_log, test_err_log)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "### Numerical Differentiation\n",
    "Please implement the **symmetric difference quotient**. Check in the exercise sheet for the formula and further details."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def numerical_differentiation(f, x):\n",
    "    \"\"\"\n",
    "    f: activation function\n",
    "    x: input\n",
    "    \"\"\"\n",
    "    h = 1e-4\n",
    "    analytical_grad = x.grad\n",
    "    \n",
    "    # To-Do: calculate the numeric gradient of the function f\n",
    "    # Hint: define two edf.Input(), one with value of x.value + h, and the other x.value - h\n",
    "    # ...\n",
    "    # ...\n",
    "    # numeric_grad = ...\n",
    "    \n",
    "    max_error = np.abs(analytical_grad - numeric_grad).max()\n",
    "    mean_error = np.abs(analytical_grad - numeric_grad).mean()\n",
    "    \n",
    "    return numeric_grad, max_error, mean_error\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "Now use the code below to show the difference between the analytical and numerical gradient.\n",
    "Firstly, we test on the `Sigmoid` function.\n",
    "\n",
    "Note that, if your `numerical_differentiation` is implemented correctly, you should see the analytical gradient and numeric gradient overlap."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "f = edf.Sigmoid\n",
    "\n",
    "def plot_gradient_difference(f):\n",
    "    values = np.linspace(-5,5,100)\n",
    "    edf.clear_compgraph()\n",
    "    x = edf.Parameter(values[None, :])\n",
    "    output = f(x)\n",
    "    edf.Forward()\n",
    "    edf.Backward(output)\n",
    "\n",
    "    # calculate numeric gradient\n",
    "    numeric_grad, max_error, mean_error = numerical_differentiation(f, x)\n",
    "\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.plot(values, output.value[0], color='red')\n",
    "    plt.plot(values, x.grad[0], color='blue', linewidth=4)\n",
    "    plt.plot(values, numeric_grad[0], color='orange')\n",
    "    plt.legend(['f', 'analytical grad', 'numeric grad'], loc='upper left')\n",
    "    plt.title('max error: %.5f,  mean error: %.5f'%(max_error, mean_error))\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "plot_gradient_difference(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "Now we visualize the gradients for `Tanh`, `ReLU` and `LeakyReLU`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_gradient_difference(Tanh)\n",
    "plot_gradient_difference(ReLU)\n",
    "plot_gradient_difference(LeakyReLU)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "## Change Learning Rate\n",
    "Let's check how changing the learning rate affects the model's train and test error. You should implement in a way to collect the network's final train/test errors for each of the learning rates in the array below. you should do this for MLPs with sigmoid and relu activations, and select the best learning rate for each of the two networks (w/ sigmoid and w/ relu activations)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Add code below to train MLPs with sigmoid activation. Your code should populate the arrays `train_err_per_lr` and `test_err_per_lr`, such that they contain the train and test errors of models trained with each learning rate in the learning_rates arrays, i.e. `train_err_per_lr[1]` should contain\n",
    "the final train error of a sigmoid MLP trained with a learning\n",
    "rate of 1.0.\n",
    "\n",
    "Hint: you should be able to reuse most of the code above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rates = [0.1, 0.5, 1.0]\n",
    "\n",
    "train_err_per_lr = []\n",
    "test_err_per_lr = []\n",
    "\n",
    "for i in range(len(learning_rates)):\n",
    "    edf.learning_rate = learning_rates[i]\n",
    "    np.random.seed(1234)\n",
    "    edf.clear_compgraph()\n",
    "\n",
    "    # TO-DO: add training code below\n",
    "    # ...\n",
    "    # ...\n",
    "    # train_err_per_lr\n",
    "    # test_err_per_lr\n",
    "\n",
    "    \n",
    "best_sigmoid_idx = np.argmin(test_err_per_lr)\n",
    "best_sigmoid_lr = learning_rates[best_sigmoid_idx]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.xlabel(\"learning rate\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(learning_rates, test_err_per_lr, color='red')\n",
    "plt.plot(best_sigmoid_lr, test_err_per_lr[best_sigmoid_idx], 'ok', label='_nolegend_')\n",
    "plt.plot(learning_rates, train_err_per_lr, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "Now repeat the experiments above but with a `ReLU` activation function in the hidden layer of a MLP. Fill below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rates = [0.1, 0.5, 1.0]\n",
    "\n",
    "train_err_per_lr = []\n",
    "test_err_per_lr = []\n",
    "\n",
    "for i in range(len(learning_rates)):\n",
    "    edf.learning_rate = learning_rates[i]\n",
    "    np.random.seed(1234)\n",
    "    edf.clear_compgraph()\n",
    "\n",
    "    # TO-DO: add training code below\n",
    "    # ...\n",
    "    # ...\n",
    "    # train_err_per_lr\n",
    "    # test_err_per_lr\n",
    "\n",
    "\n",
    "best_relu_idx = np.argmin(test_err_per_lr)\n",
    "best_relu_lr = learning_rates[best_relu_idx]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.xlabel(\"learning rate\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(learning_rates, test_err_per_lr, color='red')\n",
    "plt.plot(best_relu_lr, test_err_per_lr[best_relu_idx], 'ok', label='_nolegend_')\n",
    "plt.plot(learning_rates, train_err_per_lr, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "## Change the number of MLP hidden layers\n",
    "We will check how the number of hidden layers affects the model's performance. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Similar to the exploration with different learning rates, you will see how the depth of the network `nLayers` affects its performance, first for a sigmoid network. \n",
    "\n",
    "Your code should supports multiple hidden layers (note the new nLayers argument). Each hidden layer should have nHiddens neurons.    \n",
    "Hint: since now we might have more than one hidden layer, we can use a for loop over `nLayers` to add every layer (each intermediate `Affine` layer has `nHiddens` as both input and output)\n",
    "\n",
    "Fill the missing code below to populate `train_err_per_depth` and `test_err_per_depth` accordingly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nInputs = train_images.shape[1] # 784-dimension \n",
    "nOutputs = 10 # Output dimension\n",
    "nHiddens = 64 # Number of neurons in the hidden layer\n",
    "\n",
    "edf.learning_rate = best_sigmoid_lr\n",
    "# edf.learning_rate = 0.5\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "num_layers = [2, 4, 6]\n",
    "\n",
    "train_err_per_depth = []\n",
    "test_err_per_depth = []\n",
    "\n",
    "for nLayers in num_layers: # nLayers: number of hidden layers\n",
    "    print (\"Num Layers {}\".format(nLayers))\n",
    "    np.random.seed(1234)\n",
    "    edf.clear_compgraph()\n",
    "    \n",
    "    x_node = edf.Input()\n",
    "    y_node = edf.Input()\n",
    "    param_first = edf.AffineParams(nInputs, nHiddens)\n",
    "    h = edf.Sigmoid(edf.Affine(param_first, x_node))\n",
    "\n",
    "    # TO-DO: add code below\n",
    "    # for i in range(nLayers-1):\n",
    "    #     ...\n",
    "    #     ...\n",
    "    # prob_node = ...\n",
    "    # loss_node = ...\n",
    "    \n",
    "    # train_err_per_depth\n",
    "    # test_err_per_depth\n",
    "    \n",
    "    \n",
    "best_sigmoid_idx = np.argmin(test_err_per_depth)\n",
    "best_sigmoid_depth = num_layers[best_sigmoid_idx]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.xlabel(\"depth\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(num_layers, test_err_per_depth, color='red')\n",
    "plt.plot(num_layers, train_err_per_depth, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "Now repeat the experiment above for a ReLU MLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "edf.learning_rate = best_relu_lr\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "num_layers = [2, 4, 6]\n",
    "\n",
    "train_err_per_depth = []\n",
    "test_err_per_depth = []\n",
    "\n",
    "\n",
    "for nLayers in num_layers:\n",
    "    print (\"Num Layers {}\".format(nLayers))\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    edf.clear_compgraph()\n",
    "    x_node = edf.Input()\n",
    "    y_node = edf.Input()\n",
    "    param_first = edf.AffineParams(nInputs, nHiddens)\n",
    "    h = ReLU(edf.Affine(param_first, x_node))\n",
    "\n",
    "    # TO-DO: add code below\n",
    "    # for i in range(nLayers-1):\n",
    "    #     ...\n",
    "    #     ...\n",
    "    # prob_node = ...\n",
    "    # loss_node = ...\n",
    "    \n",
    "    # train_err_per_depth\n",
    "    # test_err_per_depth\n",
    "    \n",
    "best_relu_idx = np.argmin(test_err_per_depth)\n",
    "best_relu_depth = num_layers[best_relu_idx]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.xlabel(\"depth\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(num_layers, test_err_per_depth, color='red')\n",
    "plt.plot(num_layers, train_err_per_depth, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "## Change the Width of MLP hidden layers\n",
    "We will check how different hidden dimensions affect the model's performance. \n",
    "Fill the missing code below to populate `train_err_per_hidden` and `test_err_per_hidden` accordingly\n",
    "\n",
    "You can reuse everything you coded above. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "edf.learning_rate = best_sigmoid_lr\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "num_hidden = [32, 64, 128]\n",
    "\n",
    "nLayers = best_sigmoid_depth\n",
    "\n",
    "train_err_per_hidden = []\n",
    "test_err_per_hidden = []\n",
    "\n",
    "for nHiddens in num_hidden:\n",
    "    print (\"Hidden Dimension {}\".format(nHiddens))\n",
    "    np.random.seed(1234)\n",
    "    edf.clear_compgraph()\n",
    "    \n",
    "    x_node = edf.Input()\n",
    "    y_node = edf.Input()\n",
    "    param_first = edf.AffineParams(nInputs, nHiddens)\n",
    "    h = edf.Sigmoid(edf.Affine(param_first, x_node))\n",
    "\n",
    "    # TO-DO: add code below\n",
    "    # for i in range(nLayers-1):\n",
    "    #     ...\n",
    "    #     ...\n",
    "    # prob_node = ...\n",
    "    # loss_node = ...\n",
    "    \n",
    "    # train_err_per_hidden\n",
    "    # test_err_per_hidden\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.xlabel(\"hidden dimension\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(num_hidden, test_err_per_hidden, color='red')\n",
    "plt.plot(num_hidden, train_err_per_hidden, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "Again, Replace `Sigmoid` with `ReLU`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "edf.learning_rate = best_relu_lr\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "num_hidden = [32, 64, 128]\n",
    "\n",
    "nLayers = best_relu_depth\n",
    "\n",
    "train_err_per_hidden = []\n",
    "test_err_per_hidden = []\n",
    "\n",
    "for nHiddens in num_hidden:\n",
    "    print (\"Hidden Dimension {}\".format(nHiddens))\n",
    "    np.random.seed(1234)\n",
    "    edf.clear_compgraph()\n",
    "    \n",
    "    x_node = edf.Input()\n",
    "    y_node = edf.Input()\n",
    "    param_first = edf.AffineParams(nInputs, nHiddens)\n",
    "    h = ReLU(edf.Affine(param_first, x_node))\n",
    "\n",
    "    # TO-DO: add code below\n",
    "    # for i in range(nLayers-1):\n",
    "    #     ...\n",
    "    #     ...\n",
    "    # prob_node = ...\n",
    "    # loss_node = ...\n",
    "    \n",
    "    # train_err_per_hidden\n",
    "    # test_err_per_hidden\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.xlabel(\"hidden dimension\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(num_hidden, test_err_per_hidden, color='red')\n",
    "plt.plot(num_hidden, train_err_per_hidden, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "### Inline Question 2:\n",
    "Assume a model's train error keeps decreasing, but the test error increases when hidden dimension is too large (as shown below), what might be the reason?\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
