{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import edf\n",
    "\n",
    "dataset = np.load('2d_pcl_dataset.npz')\n",
    "X, y = dataset['X'], dataset['y']\n",
    "\n",
    "X0 = X[y==0] # 50 2D points have label 0\n",
    "X1 = X[y==1] # 50 2D points have label 1\n",
    "\n",
    "def plot(X0, X1, fit_param=None):\n",
    "    plt.scatter(X0[:,0], X0[:,1], color='red', label=0)\n",
    "    plt.scatter(X1[:,0], X1[:,1], color='blue', label=1)\n",
    "    \n",
    "    plt.xlim([-0.55, 0.55])\n",
    "    plt.ylim([-0.35, 0.25])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "plot(X0, X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the function that you need \n",
    "class SingleProbToProbVector(edf.CompNode):\n",
    "    def __init__(self, z):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.z = z\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = np.repeat(self.z.value, 2, axis=1)\n",
    "        self.value[:, 1] = 1 - self.value[:, 1]\n",
    "\n",
    "    def backward(self):\n",
    "        self.z.addgrad((self.grad[:, 0] - self.grad[:, 1]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression based on EDF for the point cloud classification task\n",
    "Use EDF to assemble a computational graph for logistic regression with the 2D points shown above as input. You will need the `SingleProbToProbVector` provided above. The function `train` below expects variables `x_node`, `y_node`, `prob_node` and `loss_node` to be defined. `prob_node` calculates the class probabilities, while the `loss_node` calculates the loss for the entire training batch, where you should use `CrossEntropyLoss`.\n",
    "\n",
    "Hint: You can reuse your code from last exercise, but now the inputs are now two-dimensional (2D points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf.clear_compgraph()\n",
    "\n",
    "# TO-DO: add your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_steps, x, y, x_node, y_node, prob_node, loss_node):\n",
    "    x_node.value = x\n",
    "    y_node.value = y\n",
    "    dataset_size = x.shape[0]\n",
    "    for iteration in range(1, num_steps + 1):\n",
    "        edf.Forward()\n",
    "        total_err = np.sum(np.not_equal(np.argmax(prob_node.value, axis=1), y_node.value))\n",
    "\n",
    "        edf.Backward(loss_node)\n",
    "        edf.UpdateParameters()\n",
    "        \n",
    "        if iteration in [100, 200, 500, 1000, 5000] or iteration % 10000 == 0:\n",
    "            print('iter: {}, error: {:6f}'.format(iteration, total_err))\n",
    "        # print(loss_node.value.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "num_steps = 100000\n",
    "learning_rate = 1\n",
    "edf.learning_rate = learning_rate\n",
    "train(num_steps, X, y, x_node, y_node, prob_node, loss_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot your result below. You should fill the `output` with your trained logistic model.\n",
    "\n",
    "Hint: you can re-use your code when assembling the computational graph, but remove `SingleProbToProbVector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linspace(-0.5, 0.5, 500)\n",
    "x_plot, y_plot = np.meshgrid(p, p)\n",
    "X_plot = np.stack((x_plot, y_plot), axis=2).reshape(-1, 2)\n",
    "edf.clear_compgraph()\n",
    "input = edf.Parameter(X_plot)\n",
    "\n",
    "#To-DO: fill code here\n",
    "# output = ...\n",
    "output = edf.Sigmoid(edf.Affine(affine_params, input))\n",
    "edf.Forward()\n",
    "edf.Backward(output)\n",
    "\n",
    "output = output.value.reshape(x_plot.shape[0], x_plot.shape[1])\n",
    "plt.contour(x_plot, y_plot, output, [0.5],\n",
    "                  colors=('k',),\n",
    "                  linewidths=(3,))\n",
    "plot(X0, X1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement your first multi-layer perceptron (MLP) for point cloud classification\n",
    "Now you should notice that your logistic regression model cannot classify correctly, and this was taught in the lecture.  \n",
    "Therefore, we now ask you to implement an MLP for the same binary classification task.\n",
    "\n",
    "Again, you need to define `x_node`, `y_node`, `prob_node` and `loss_node` below.\n",
    "\n",
    "Hint: unlike logistic regression where you need only one affine layer, here you will have two affine layers. After the first affine layer, we also apply a Sigmoid function. The output is then passed to another affine layer followed by another Sigmoid function that directly outputs the final probability. Note that you should use `nHiddens` below as the hidden dimension of your affine payers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf.clear_compgraph()\n",
    "nHiddens = 16\n",
    "\n",
    "# TO-DO: add your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run again\n",
    "np.random.seed(1234)\n",
    "\n",
    "num_steps = 100000\n",
    "learning_rate = 1\n",
    "edf.learning_rate = learning_rate\n",
    "train(num_steps, X, y, x_node, y_node, prob_node, loss_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again plot your result. You should fill the `output` with your trained MLP model.\n",
    "\n",
    "Hint: the `output` should be your `prob_node`, but the `x_node` should be replaced by `input`, and remove `SingleProbToProbVector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linspace(-0.5, 0.5, 500)\n",
    "x_plot, y_plot = np.meshgrid(p, p)\n",
    "X_plot = np.stack((x_plot, y_plot), axis=2).reshape(-1, 2)\n",
    "edf.clear_compgraph()\n",
    "input = edf.Parameter(X_plot)\n",
    "\n",
    "#To-DO: fill code here\n",
    "# output = ...\n",
    "\n",
    "edf.Forward()\n",
    "edf.Backward(output)\n",
    "\n",
    "output = np.round(output.value).reshape(x_plot.shape[0], x_plot.shape[1])\n",
    "plt.contour(x_plot, y_plot, output, [0.5],\n",
    "                  colors=('k',),\n",
    "                  linewidths=(3,))\n",
    "plot(X0, X1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
